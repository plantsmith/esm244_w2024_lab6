---
title: "NLS Lab Key"
author: "Nathan Grimes, Casey O'Hara"
format: 
  html:
    code-fold: true
    embed-resources: true
execute: 
  echo: true
  message: false
  warning: false
editor: visual
---

## Introduction

To demonstrate the power of non linear least squares in R we're going to recreate a fisheries paper that examined whether productivity in fisheries was driven more by abundance, regime shifts, or simply random noise. [Here is a link to the paper if curious about detailed methods and results](https://www.pnas.org/content/110/5/1779). In their research, they used maximum likelihood estimation rather than non-linear least squares, but we will see similar results. In fact, the model choices for nls and mle are nearly identical in selected coefficients! Also to simplify the lab, we will only recreate the abundance and random models.

### Data

All data comes from the [RAM Legacy Database](https://www.re3data.org/repository/r3d100012095), normally we would have to go through the whole database, but instead I have extracted out the main table containing all the values of stock parameters and a list of stocks within the cod and sole families to examine. Anyone interested in fisheries should familiarize themselves with RAM because it is the best set of fisheries data across different stocks, countries, and time.

Load in the data.

```{r}
load(here::here("data","fish_data.Rdata"))

library(tidyverse)
library(Metrics)
library(cowplot)
library(here)
```

## Single model NLS

### Step 0: Planning our attack

1.  Understand Model

2.  Apply NLS to single fish stock

```         
a.   Model Selection

b.   Create R Function

c.  Define initial Guess

d.  Run NLS

e.  Evaluate Results
```

3.  Use purrr to map nls to all stocks

4.  Build a null model and use purrr for all stocks

5.  Compare Fox Model to Null Model

6.  Plot results

### Step One: Selecting a Model

Surplus is the excess amount of biomass that was added or taken from the underlying stock. It can be modeled as a simple addition. Surplus also allows us to model recruitment, growth, and natural mortality that is often difficult data to collect. Stock assessments, that RAM is built on, allows us to easily back out surplus using historical catch and biomass records.

```{=tex}
\begin{equation}
S_t=B_{t+1}-B_t+C_t
\end{equation}
```
We will need to add a column in our dataset calculating surplus in any given year. Since we have a variable from the future we can use the `lead()` function. Make sure to drop the `NA` created by the lead function.

```{r}
surplus<-Fish_data %>% 
  group_by(stockid) %>% 
  select(stockid,year,TBbest,TCbest) %>% 
  drop_na() %>% 
  mutate(f_biomass=lead(TBbest)) %>% 
  mutate(surplus=f_biomass-TBbest+TCbest) %>% 
  drop_na()
  
  
```

Let's see what our data looks like with an example of one stock.

```{r}
one_stock<-surplus %>% 
  filter(stockid=="COD1f-XIV")

ggplot(data=one_stock,aes(x=year,y=surplus))+
  geom_point(size=3,color="black")+
  theme_minimal()

```

Non-linear least squares needs a model to fit the data. Understanding which model to use relies on your expertise and ability to justify its selection. Since we're trying to predict fisheries surplus, we should examine the surplus-production literature.

There are three primary surplus-production models in the fishery world. The most common is the Gordon-Schaefer model. Vert-pre et al., use a Fox-Model that typically provides a more conservative estimate of maximum sustainable yield. The last model is the Pella-Tomlinson model that really is just a more flexible model of the other two using a shape parameter $\phi$ to control the curve. All are built on a logistic growth curve. Given a level of biomass we will be able to predict what the surplus ought to be if we know (or will determine) the maximum sustainable yield and the carrying capacity. Maximum sustainable yield simply refers to the amount of biomass that facilitates the greatest level of harvest possible without depleting the stock. Carrying capacity is the upper bound on the total population size and represents natural environmental pressure limiting stock growth. The paper uses a simplified Fox model that we try to find parameters for to fit the fishery data.

```{=tex}
\begin{equation}
\hat{S_t}=-e*MSY(\frac{B_t}{K})\ln(\frac{B_t}{K})
\end{equation}
```
Where e is base of the natural log $\approx$ 2.718, MSY is the maximum sustainable yield, K is the carrying capacity, and $B_t$ is the biomass for the observed year.

Let's create a function in R.

```{r foxmodel}
fox<-function(m,carry,biomass){
 out= -2.718*m*(biomass/carry)*log(biomass/carry)
return(out)
}
```

```{r}
  
carry=150000

m=60000

fox_sim<-one_stock |>   
  mutate(predict=fox(m,carry,TBbest))

ggplot(data=fox_sim,aes(x=year,y=surplus,color="Data"))+
  geom_line(aes(x=year,y=predict,color="Model"),linewidth=2)+
  geom_point(size=3)+
  scale_color_manual(values=c("Data"="black","Model"="red"))+
  labs(x="",y="Surplus Production")+
  theme_minimal()

```

### Step Three: Initial Guess

Now we can construct our nonlinear least squares with sufficient guesses. But what should our guesses be? Traditionally, carrying capacity is estimated as the highest observed biomass so we can just take the max of the biomass data. Maximum sustainable yield can be found analytically. It's been done many times so I'll just tell you it's estimated at 37% of the carrying capacity.

```{r guess}

#Write out the guess first, we'll move into the nls wrapper soon

#Since MSY comes into the fox function first, it needs to be the first guess

guess_vec=c(max(one_stock$TBbest)*0.37,
            max(one_stock$TBbest))

```

### Step Four: Run NLS

Fill in the gaps in our nls function to make it run!

```{r nlsonemodel}
one_stock_nls=nls(surplus~fox(m,carry,TBbest),
                  data=one_stock,
                  start=list(m=guess_vec[1],carry=guess_vec[2]),
                  trace=TRUE)
```

### Step Five: Evaluate

Show how the model fits the data.

```{r}
# Make a predict based on the model

one_stock_predict<-one_stock %>% 
  mutate(predict=predict(one_stock_nls,newdata=.))

ggplot(data=one_stock_predict)+
  geom_point(aes(x=year,y=surplus))+
  geom_path(aes(x=year,y=predict),color='red')
```

Great our model works on a single model! Now we need to find a way to replicate the analysis. Ideally without using for loops as those can be a pain to account for.

## Using purrr to run many nls models

```{r nlsmany}
#Define a new function to pass along the nls calls

all_nls_fcn<-function(surplus_df){
  nls(surplus~fox(m,carry,TBbest),
  data=surplus_df,
  start=list(m=max(surplus_df$TBbest)*0.37,carry=max(surplus_df$TBbest)))
}

## Pay attention to the position and use of .x, .y, and .f in the map functions


fox_all<-surplus %>%
  group_by(stockid) %>% 
  nest() %>% 
  mutate(nls_model=map(data,~all_nls_fcn(.x))) %>% 
  mutate(predictions=map2(nls_model,data,~predict(.x,newdata=.y))) %>% 
  mutate(RMSE=map2_dbl(predictions,data,~rmse(.x,.y$surplus)))

```

## Compare to a random null model

In the paper, they derive a null model to test the different models against. The best way to test if any of these models are better is if they can out perform a random collection of data. They propose if our models can't outperform the average surplus in the time frame, then the stock is under more influence of sheer randomness then any explicable measures. We can jump straight into the purrr analysis.

```{r}

# Define the model, don't worry to much how I got it what it means
r_avg<-function(surplus){
  avg_sur=mean(surplus)
  
  rmse=sqrt(mean((avg_sur-surplus)^2))
  
  return(rmse)
}


r_mse<-surplus %>%
  group_by(stockid) %>% 
  nest() %>% 
  mutate(RMSE=map_dbl(data,~r_avg(.x$surplus)))
```

## How did the models compare to the null?

```{r}
which(r_mse$RMSE-fox_all$RMSE<0)

fox_all$stockid[39]
```

In the paper, about 12% of the stocks were more explained by random shocks and 16% more so by abundance models. The rest were led by regime shifts that we did not model. Our results only found one stock out of 44 was better explained by random growth. Either our choice of nls models is far better than their mle method (not likely), or the subset I chose lends itself more to abundance models (this is really what happened). I did not want to overwhelm the analysis with over 200 stocks. Cod species in their paper typically were best explained by abundance models. Since our dataset focuses on cod, it's unsurprising that the Fox model best predicts cod surplus.

## Graph the top 5 best fit models

Purrr combined with cowplot creates a streamlined way to build multiple graphs. Let's take the 5 best fit Fox models and show how they performed compared to the data.

```{r}
plots<-fox_all |> 
  arrange(RMSE) |>  
  head(5) |> 
  mutate(graph=map2(data,predictions,
                    ~ggplot()+
                      geom_point(data = .x,aes(x=.x$year,y=.x$surplus,color='Actual'))+
                      geom_point(aes(x=.x$year,y=.y,color='Predicted'))+
                      theme_minimal()+xlab('')+ylab('Surplus')+
                      scale_color_manual(name="Legend",breaks = c('Actual','Predicted'),values=c('Actual'='black','Predicted'='red'))))

#Make a new list
plot_list=plots$graph

cowplot::plot_grid(plotlist=plot_list,labels =c( plots$stockid,""),hjust=-0.5,vjust=1)


```

### Extra Cowplot fun to make it look really clean

```{r legends}
#extract the legend

legend<-get_legend(plots$graph[[1]])

#remove the legend from the plots
# sometimes you don't always need to use purrr, a quick for loop for something easy can also work
for(i in 1:length(plots$graph)){
  plots$graph[[i]]<-plots$graph[[i]]+theme(legend.position = "none")
}

#Make a new list
plot_list_legend=plots$graph

#create a new plot in the empty space of cowplot grids and fill it with the legend info we took from the plots.

plot_list_legend[[6]]<-legend

cowplot::plot_grid(plotlist=plot_list_legend,labels =c( plots$stockid,""),hjust=-0.5,vjust=1)
```
